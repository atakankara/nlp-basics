{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "**Reference:** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. ([Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks), [Sample code](https://github.com/tensorflow/nmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@size (macro with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, IterTools, Random, LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, use with Float64\n",
    "using Statistics\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    # Your code here\n",
    "    counts_dict = Dict{String,Int}()\n",
    "    w2i_dict = Dict{String,Int}()\n",
    "    i2w = Vector{String}()\n",
    "\n",
    "    w2i_function(x) = get!(w2i_dict, x, 1+length(w2i_dict))\n",
    "\n",
    "    for line in eachline(file)\n",
    "        for token in tokenizer(line)\n",
    "            if haskey(counts_dict, token)\n",
    "                counts_dict[token] += 1\n",
    "            else\n",
    "                counts_dict[token] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    if haskey(counts_dict, unk); delete!(counts_dict, unk) end\n",
    "    if haskey(counts_dict, eos); delete!(counts_dict, eos) end\n",
    "\n",
    "    for (token, count) in sort!(collect(counts_dict), by=x->x[2], rev=true)[1:Int(min(length(counts_dict), vocabsize-2))]\n",
    "        if count >= mincount\n",
    "            w2i_function(token)\n",
    "            push!(i2w, token)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    w2i_function(unk)\n",
    "    push!(i2w, unk)    \n",
    "    w2i_function(eos)\n",
    "    push!(i2w, eos)    \n",
    "\n",
    "    Vocab(w2i_dict, i2w, w2i_dict[unk], w2i_dict[eos], tokenizer)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    # Your code here\n",
    "    \n",
    "    if s == nothing\n",
    "        s = open(r.file)\n",
    "    end\n",
    "    \n",
    "    if !eof(s)\n",
    "        return s |> readline |> split |> (x-> [get(r.vocab.w2i, token, r.vocab.unk) for token in x]), s\n",
    "    else\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    # Your code here\n",
    "    Embed(param(embedsize, vocabsize, init=xavier))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    # Your code here\n",
    "    l.w[:, x]\n",
    "end\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    # Your code here\n",
    "    Linear(param(outputsize, inputsize, init=xavier), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    # Your code here\n",
    "    l.w*x .+ l.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mask!(a,pad)\n",
    "    # Your code here\n",
    "    for r in 1:size(a)[1]\n",
    "        column_length = size(a)[2]\n",
    "        for c in 1:column_length\n",
    "            if a[r, column_length-c+1] != pad\n",
    "                a[r, column_length-c+3:end] .= 0\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Load data\n",
    "\n",
    "We will use the Turkish-English pair from the [TED Talks Dataset](https://github.com/neulab/word-embeddings-for-nmt) for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing data\n",
      "└ @ Main In[6]:17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: length(collect(tr_test)) == 5029\n",
       "   Evaluated: 5029 == 5029"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"datasets/tr_to_en\"\n",
    "\n",
    "if !isdir(datadir)\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    tr_vocab = Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    @info \"Testing data\"\n",
    "    @test length(tr_vocab.i2w) == 38126\n",
    "    @test length(first(tr_test)) == 16\n",
    "    @test length(collect(tr_test)) == 5029\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Minibatching\n",
    "\n",
    "For minibatching we are going to design a new iterator: `MTData`. This iterator is built\n",
    "on top of two TextReaders `src` and `tgt` that produce parallel sentences for source and\n",
    "target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar source sentence length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate(::MTData)\n",
    "\n",
    "Define the `iterate` function for the `MTData` iterator. `iterate` should return a\n",
    "`(batch, state)` pair or `nothing` if there are no more batches.  The `batch` is a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. The `state` is a pair of `(src_state,tgt_state)` which can be used\n",
    "to iterate `d.src` and `d.tgt` to get more sentences.  `iterate(d)` without a second\n",
    "argument should initialize `d` by emptying its buckets and calling `iterate` on the inner\n",
    "iterators `d.src` and `d.tgt` without a state. Please review the documentation on\n",
    "iterators from the last project.\n",
    "\n",
    "To keep similar length sentences together `MTData` uses arrays of similar length sentence\n",
    "pairs called buckets.  Specifically, the `(src_sentence,tgt_sentence)` pairs coming from\n",
    "`src` and `tgt` are pushed into `d.buckets[i]` when the length of the source sentence is\n",
    "in the range `((i-1)*d.bucketwidth+1):(i*d.bucketwidth)`. When one of the buckets reaches\n",
    "`d.batchsize` `d.batchmaker` is called with the full bucket producing a 2-D batch, the\n",
    "bucket is emptied and the batch is returned. If `src` and `tgt` are exhausted the\n",
    "remaining partially full buckets are turned into batches and returned in any order. If the\n",
    "source sentence length is larger than `length(d.buckets)*d.bucketwidth`, the last bucket\n",
    "is used.\n",
    "\n",
    "Sentences above a certain length can be skipped using the `d.maxlength` field, and\n",
    "transposed `x,y` arrays can be produced using the `d.batchmajor` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    if state == nothing\n",
    "        #Clean buckets\n",
    "        for i in 1:length(d.buckets)\n",
    "            d.buckets[i] = []\n",
    "        end\n",
    "        #Initialize TextReaders\n",
    "        state = Vector{Any}([nothing, nothing])\n",
    "        iterate(d.src)\n",
    "        iterate(d.tgt)\n",
    "    end\n",
    "    \n",
    "    #Return remaining buckets if one of the sources are exhausted.\n",
    "    if state isa Number\n",
    "        for j in state+1:length(d.buckets)\n",
    "            if !isempty(d.buckets[j])\n",
    "                return d.batchmaker(d, d.buckets[j]), j\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return nothing\n",
    "    end\n",
    "        \n",
    "    while true\n",
    "        src_sentence, state[1] = iterate(d.src, state[1])\n",
    "        tgt_sentence, state[2] = iterate(d.tgt, state[2])\n",
    "\n",
    "        i = min(ceil(Int8, length(src_sentence) / d.bucketwidth), length(d.buckets))\n",
    "        push!(d.buckets[i], (src_sentence,tgt_sentence))\n",
    "        \n",
    "        #If one of the resources is exhausted, start returning batch indices.\n",
    "        if eof(state[1]) || eof(state[2])\n",
    "            for j in 1:length(d.buckets)\n",
    "                if !isempty(d.buckets[j])\n",
    "                   return d.batchmaker(d, d.buckets[j]), j\n",
    "                end\n",
    "            end\n",
    "            return nothing\n",
    "        end\n",
    "        \n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            batch = d.batchmaker(d, d.buckets[i])\n",
    "            d.buckets[i] = []\n",
    "            return batch, state\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #Skip long source sentences.\n",
    "        if length(src_sentence) > d.maxlength\n",
    "            continue\n",
    "        end      \n",
    "        \n",
    "        \n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arraybatch\n",
    "\n",
    "Define `arraybatch(d, bucket)` to be used as the default `d.batchmaker`. `arraybatch`\n",
    "takes an `MTData` object and an array of sentence pairs `bucket` and returns a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. Note that the sentences in the bucket do not have any `eos` tokens\n",
    "and they may have different lengths. `arraybatch` should copy the source sentences into\n",
    "`x` padding shorter ones on the left with `eos` tokens. It should copy the target\n",
    "sentences into `y` with an `eos` token in the beginning and end of each sentence and\n",
    "shorter sentences padded on the right with extra `eos` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function arraybatch(d::MTData, bucket)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    srclength = 0\n",
    "    tgtlength = 0 \n",
    "    for i in 1:length(bucket)\n",
    "        if length(bucket[i][1]) > srclength; srclength = length(bucket[i][1]) end\n",
    "        if length(bucket[i][2]) > tgtlength; tgtlength = length(bucket[i][2]) end\n",
    "    end  \n",
    "    \n",
    "    tgtlength += 2 #Because of the additional eos tokens in both of the sides.\n",
    "    \n",
    "    for i in 1:length(bucket)\n",
    "        padded_x = repeat([d.src.vocab.eos], srclength - length(bucket[i][1]))\n",
    "        append!(padded_x, deepcopy(bucket[i][1]))\n",
    "\n",
    "        padded_y = [d.tgt.vocab.eos]\n",
    "        append!(padded_y, deepcopy(bucket[i][2]))\n",
    "        append!(padded_y, d.tgt.vocab.eos)\n",
    "        \n",
    "        append!(padded_y, repeat([d.tgt.vocab.eos], tgtlength - length(padded_y)))       \n",
    "        \n",
    "        push!(x, padded_x)\n",
    "        push!(y, padded_y)\n",
    "    end\n",
    "    if d.batchmajor\n",
    "        return permutedims(hcat(x...))', permutedims(hcat(y...))'\n",
    "    else\n",
    "        return permutedims(hcat(x...)), permutedims(hcat(y...))\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing MTData\n",
      "└ @ Main In[10]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: y[1, end] == en_vocab.eos\n",
       "   Evaluated: 18857 == 18857"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing MTData\"\n",
    "dtrn = MTData(tr_train, en_train)\n",
    "ddev = MTData(tr_dev, en_dev)\n",
    "dtst = MTData(tr_test, en_test)\n",
    "\n",
    "x,y = first(dtst)\n",
    "@test length(collect(dtst)) == 48\n",
    "@test size.((x,y)) == ((128,10),(128,24))\n",
    "@test x[1,1] == tr_vocab.eos\n",
    "@test x[1,end] != tr_vocab.eos\n",
    "@test y[1,1] == en_vocab.eos\n",
    "@test y[1,2] != en_vocab.eos\n",
    "@test y[1,end] == en_vocab.eos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Sequence to sequence model without attention\n",
    "\n",
    "In this part we will define a simple sequence to sequence encoder-decoder model for\n",
    "machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    srcvocab::Vocab     # source language vocabulary\n",
    "    tgtvocab::Vocab     # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 constructor\n",
    "\n",
    "Define the S2S_v1 constructor using your predefined layer types (Embed, Linear), and the\n",
    "Knet RNN type. Please review the RNN documentation using `@doc RNN`, paying attention to\n",
    "the following options in particular: `numLayers`, `bidirectional`, `dropout`, `atype`.\n",
    "The last one is important if you experiment with array types other than the\n",
    "default `KnetArray{Float32}`: make sure the RNNs use the same array type as the other\n",
    "layers. Note that if the encoder is bidirectional, its `numLayers` should be half of the\n",
    "decoder so that their hidden states match in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S_v1(hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "                srcembsz::Int,       # embedding size for source language\n",
    "                tgtembsz::Int,       # embedding size for target language\n",
    "                srcvocab::Vocab,     # vocabulary for source language\n",
    "                tgtvocab::Vocab;     # vocabulary for target language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "    if bidirectional\n",
    "          encoder = RNN(srcembsz, hidden; bidirectional=true, numLayers=layers/2, dropout=dropout)\n",
    "    else\n",
    "          encoder = RNN(srcembsz, hidden; bidirectional=false, numLayers=layers, dropout=dropout)\n",
    "    end\n",
    "\n",
    "\n",
    "    S2S_v1(Embed(length(srcvocab.i2w), srcembsz),\n",
    "              encoder,\n",
    "              Embed(length(tgtvocab.i2w), tgtembsz),\n",
    "              RNN(tgtembsz, hidden; numLayers=layers, dropout=dropout),\n",
    "              Linear(hidden, size(tgtvocab.i2w)[1]),\n",
    "              dropout,\n",
    "              srcvocab,\n",
    "              tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 loss function\n",
    "\n",
    "Define the S2S_v1 loss function that takes `src`, a source language minibatch, and `tgt`,\n",
    "a target language minibatch and returns either a `(total_loss, num_words)` pair if\n",
    "`average=false`, or `(total_loss/num_words)` average if `average=true`.\n",
    "\n",
    "Assume that `src` and `tgt` are integer arrays of size `(B,Tx)` and `(B,Ty)` respectively,\n",
    "where `B` is the batch size, `Tx` is the length of the longest source sequence, `Ty` is\n",
    "the length of the longest target sequence. The `src` sequences only contain words, the\n",
    "`tgt` sequences surround the words with `eos` tokens at the start and end. This allows\n",
    "columns `tgt[:,1:end-1]` to be used as the decoder input and `tgt[:,2:end]` as the desired\n",
    "decoder output.\n",
    "\n",
    "Assume any shorter sentences in the batches have been padded with extra `eos` tokens on\n",
    "the left for `src` and on the right for `tgt`. Don't worry about masking `src` for the\n",
    "encoder, it doesn't have a significant effect on the loss. However do mask `tgt` before\n",
    "`nll`: you do not want the padding tokens to be counted in the loss calculation.\n",
    "\n",
    "Please review `@doc RNN`: in particular the `r.c` and `r.h` fields can be used to get/set\n",
    "the cell and hidden arrays of an RNN (note that `0` and `nothing` act as special values).\n",
    "\n",
    "RNNs take a dropout value at construction and apply dropout to the input of every layer if\n",
    "it is non-zero. You need to handle dropout for other layers in the loss function or in\n",
    "layer definitions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src, tgt; average=true)\n",
    "    src_embeddings = s.srcembed(src)\n",
    "    tgt_embeddings = s.tgtembed(tgt[:,1:end-1])\n",
    "    \n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "\n",
    "    encoder_hidden_states = s.encoder(src_embeddings)\n",
    "\n",
    "    s.decoder.c = s.encoder.c\n",
    "    s.decoder.h = s.encoder.h\n",
    "            \n",
    "    decoder_hidden_states = s.decoder(tgt_embeddings)\n",
    "\n",
    "    prediction = s.projection(reshape(decoder_hidden_states, size(decoder_hidden_states, 1), :))\n",
    "    \n",
    "    masked_tgt = tgt[:, 2:end]\n",
    "    mask!(masked_tgt, s.tgtvocab.eos)\n",
    "\n",
    "    return nll(reshape(prediction, size(prediction, 1), size(masked_tgt, 1), size(masked_tgt,2)), \n",
    "                masked_tgt, \n",
    "                average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14097.171f0, 1432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[17]:1\n"
     ]
    }
   ],
   "source": [
    "@info \"Testing S2S_v1\"\n",
    "Random.seed!(1)\n",
    "model = S2S_v1(512, 512, 512, tr_vocab, en_vocab; layers=2, bidirectional=true, dropout=0.2)\n",
    "(x,y) = first(dtst)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "\n",
    "# My results were (14096.201f0, 1432) and I'm using different vocabulary ordering.\n",
    "println(model(x,y; average=false))\n",
    "# @test model(x,y; average=false) == (14096.252f0, 1432) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for a whole dataset\n",
    "\n",
    "Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
    "iterator of `(x,y)` pairs such as `MTData` and `model(x,y;average)` is a model like\n",
    "`S2S_v1` that computes loss on a single `(x,y)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    # Your code here\n",
    "    counts = 0\n",
    "    total = 0\n",
    "    for (x, y) in data\n",
    "        (tot, cnt) = model(x, y, average = false)\n",
    "        total += tot\n",
    "        counts += cnt\n",
    "    end\n",
    "    if average\n",
    "        return total/counts\n",
    "    else\n",
    "        return total, counts\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.04299156f6, 105937)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss\n",
      "└ @ Main In[16]:1\n"
     ]
    }
   ],
   "source": [
    "@info \"Testing loss\"\n",
    "println(loss(model, dtst, average=false))\n",
    "# @test loss(model, dtst, average=false) == (1.0427646f6, 105937)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "# Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "# losses. The test above gives (1.0430301f6, 105937) with batchsize==1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SGD_v1\n",
    "\n",
    "The following function can be used to train our model. `trn` is the training data, `dev`\n",
    "is used to determine the best model, `tst...` can be zero or more small test datasets for\n",
    "loss reporting. It returns the model that does best on `dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=100) do y\n",
    "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
    "        println(losses)\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get under 3.40 dev loss with the following settings in 10\n",
    "epochs. The training speed on a V100 is about 3 mins/epoch or 40K words/sec, K80 is about\n",
    "6 times slower. Using settings closer to the Luong paper (per-sentence loss rather than\n",
    "per-word loss, SGD with lr=1, gclip=1 instead of Adam), you can get to 3.17 dev loss in\n",
    "about 25 epochs. Using dropout and shuffling batches before each epoch significantly\n",
    "improve the dev loss. You can play around with hyperparameters but I doubt results will\n",
    "get much better without attention. To verify your training, here is the dev loss I\n",
    "observed at the beginning of each epoch in one training session:\n",
    "`[9.83, 4.60, 3.98, 3.69, 3.52, 3.41, 3.35, 3.32, 3.30, 3.31, 3.33]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Training S2S_v1\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trnx10 = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "trn20 = ctrn[1:20]\n",
    "dev38 = collect(ddev)\n",
    "# Uncomment this to train the model (This takes about 30 mins on a V100, 60 mins on a T4):\n",
    "model = train!(model, trnx10, dev38, trn20);\n",
    "# Uncomment this to save the model:\n",
    "# Knet.save(\"s2s_v1.jld2\",\"model\",model)\n",
    "# Uncomment this to load the model:\n",
    "# model = Knet.load(\"s2s_v1.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knet.save(\"s2s_v1.jld2\",\"model\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1(Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(512,38126))), LSTM(input=512,hidden=512,bidirectional,dropout=0.2), Embed(P(Knet.KnetArrays.KnetMatrix{Float32}(512,18857))), LSTM(input=512,hidden=512,layers=2,dropout=0.2), Linear(P(Knet.KnetArrays.KnetMatrix{Float32}(18857,512)), P(Knet.KnetArrays.KnetVector{Float32}(18857))), 0.2, Vocab(Dict(\"ağacından\" => 35368, \"komuta\" => 13564, \"ellisi\" => 25237, \"adresini\" => 22818, \"yüzeyi\" => 4049, \"paris'te\" => 9492, \"kafamdaki\" => 18788, \"yüzeyinde\" => 5040, \"geçerlidir\" => 6610, \"kökten\" => 7772…), [\".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\", \"çok\", \"da\"  …  \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\", \"<unk>\", \"<s>\"], 38125, 38126, split), Vocab(Dict(\"offend\" => 14330, \"hyperbole\" => 18615, \"enjoy\" => 1730, \"advertisements\" => 10004, \"chocolate\" => 3321, \"nicholas\" => 9023, \"tolstoy\" => 17601, \"uniformly\" => 18616, \"favorites\" => 5269, \"middle-income\" => 13396…), [\",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\", \"i\", \"in\"  …  \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\", \"<unk>\", \"<s>\"], 18856, 18857, split))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Knet.load(\"s2s_v1.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating translations\n",
    "\n",
    "With a single argument, a `S2S_v1` object should take it as a batch of source sentences\n",
    "and generate translations for them. After passing `src` through the encoder and copying\n",
    "its hidden states to the decoder, the decoder is run starting with an initial input of all\n",
    "`eos` tokens. Highest scoring tokens are appended to the output and used as input for the\n",
    "subsequent decoder steps.  The decoder should stop generating when all sequences in the\n",
    "batch have generated `eos` or when `stopfactor * size(src,2)` decoder steps are reached. A\n",
    "correctly shaped target language batch should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    # Your code here\n",
    "    src_embeddings = s.srcembed(src)\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "      \n",
    "    encoder_hidden_states = s.encoder(src_embeddings)\n",
    "\n",
    "    s.decoder.c = s.encoder.c\n",
    "    s.decoder.h = s.encoder.h\n",
    "    \n",
    "    prediction = reshape(hcat(repeat([s.tgtvocab.eos], size(src, 1))...), size(src, 1), 1) #All eos tags.\n",
    "    prediction_embeddings = s.tgtembed(prediction)\n",
    "#     prediction_embeddings = reshape(prediction_embeddings, size(prediction_embeddings, 1), size(prediction_embeddings,3), size(prediction_embeddings, 2))\n",
    "    generated_words = prediction #Start with eos for each batch\n",
    "\n",
    "    for i in 1:(stopfactor * size(src,2))\n",
    "        decoder_hidden_states = s.decoder(prediction_embeddings);\n",
    "        scores = s.projection(reshape(decoder_hidden_states, size(decoder_hidden_states, 1), :))\n",
    "        prediction = [x[1] for x in argmax(scores, dims=1)]\n",
    "        generated_words = hcat(generated_words, prediction')\n",
    "        prediction_embeddings = s.tgtembed(prediction)\n",
    "        prediction_embeddings = reshape(prediction_embeddings, size(prediction_embeddings,1), size(prediction_embeddings, 3), size(prediction_embeddings, 2))\n",
    "        \n",
    "        if (sum(prediction .== s.tgtvocab.eos) == size(prediction, 1))\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    generated_words[:, 2:end]    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int2str (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run these lines if you get a \"CUDNNError: CUDNN_STATUS_INTERNAL_ERROR (code 4)\" error from the cell below.\n",
    "# Knet.save(\"s2s_v1.jld2\",\"model\",model)\n",
    "# model = Knet.load(\"s2s_v1.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: alice hastalarına çok sıcak , çok anlayışlı <unk> .\n",
      "REF: alice was very warm , very empathetic with her patients .\n",
      "OUT: and the adults who are very angry , too many , very much for the <unk> .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Generating some translations\n",
      "└ @ Main In[25]:1\n"
     ]
    }
   ],
   "source": [
    "@info \"Generating some translations\"\n",
    "d = MTData(tr_dev, en_dev, batchsize=1) |> collect\n",
    "(src,tgt) = rand(d)\n",
    "out = model(src)\n",
    "println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "# Here is a sample output:\n",
    "# SRC: çin'e 15 şubat 2006'da ulaştım .\n",
    "# REF: i made it to china on february 15 , 2006 .\n",
    "# OUT: i got to china , china , at the last 15 years ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating BLEU\n",
    "\n",
    "BLEU is the most commonly used metric to measure translation quality. The following should\n",
    "take a model and some data, generate translations and calculate BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating dev BLEU takes about 100 secs on a T4. We get about 8.0 BLEU which is pretty\n",
    "low. As can be seen from the sample translations a loss of ~3+ (perplexity ~20+) or a BLEU\n",
    "of ~8 is not sufficient to generate meaningful translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Calculating BLEU\n",
      "└ @ Main In[27]:1\n",
      "┣████████████████████┫ [100.00%, 4045/4045, 02:43/02:43, 24.81i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 7.50, 36.5/11.1/4.3/1.8 (BP=1.000, ratio=1.079, hyp_len=89054, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\atakan\\\\AppData\\\\Local\\\\Temp\\\\jl_66F2.tmp\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Calculating BLEU\"\n",
    "bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality of translations we can use more training data, different training\n",
    "and model parameters, or preprocess the input/output: e.g. splitting Turkish words to make\n",
    "suffixes look more like English function words may help. Other architectures,\n",
    "e.g. attention and transformer, perform significantly better than this simple S2S model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
